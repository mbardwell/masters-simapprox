{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Determine cost of running PyPSA simulation vs evaluating a trained ANN model on one sample\n",
    "\n",
    "**Experimental Procedure**:\n",
    "\n",
    "1. Time running a pf simulation with 1 data point\n",
    "2. Train approximation models\n",
    "3. Time evaluating models with 1 data point\n",
    "\n",
    "**Hypothesis**: \n",
    "\n",
    "The time required to evaluate the models will be insignificant when compared to simulating one sample. To calculate 1 data point using the simulation the computer enlists a [Newton-Raphson (NR) root-finding algorithm](https://www.math.ubc.ca/~anstee/math104/newtonmethod.pdf),\n",
    "\n",
    "x<sub>n+1</sub> = x<sub>n</sub> - $\\frac{f(x_n)}{f'(x_n)}$,\n",
    "\n",
    "as well as other miscellaneous PyPSA operations. The total cost being,\n",
    "\n",
    "$time_{running-sim}$(f(x)) = time_NR_algorithm + time_PyPSA_related_operations.\n",
    "\n",
    "It would be difficult to evaluate $time_{running-sim}$(f(x)) analytically, so an experimental result is used. Using simple speed tests for running a simulation on one sample (this sample requires 4 NR iterations),\n",
    "\n",
    "$time_{running-sim}$(f(x)) = **3e-1 s**.\n",
    "\n",
    "\n",
    "\n",
    "The math to evaluate an ANN model can be written in one line, and therefore an analytical estimate is presented. To evaluate an ANN model of four fully-connected layers (1 input [13 features], 2 hidden [sigmoid, 100 neurons per], 1 output [linear, 9 labels]) with one sample,\n",
    "\n",
    "f(x) = weighted_sum_l1_l2 + evaluate_sigmoid_l2 + weighted_sum_l2_l3 + evaluate_sigmoid_l3 + weighted_sum_l3_l4\n",
    "\n",
    "$time_{model-evaluation}$(f(x)) = 13 * 100 * C1 + 100 * C2 + 100 * 100 * C1 + 100 * C2 + 100 * 9 * C1 = 12100 * C1 + 200 * C2\n",
    "\n",
    "Where, C1 is the cost of one multiplication operation and C2 is the cost of one logistic sigmoid evaluation. Basic speed tests set the values at C1 = 5e-8s, C2 = 2e-6s (using _scipy's expit_ function) so,\n",
    "\n",
    "$time_{model-evaluation}$(f(x)) ~ 12100 * 5e-8 + 200 * 2e-6 = 6e-4 + 4e-4 = **1e-3 s**.\n",
    "\n",
    "\n",
    "**In summary, model evaluation is expected to take two orders of magnitude less time than running a simulation**\n",
    "\n",
    "**Results**:\n",
    "* Single sample power flow simulation time: 0.3\n",
    "* ann evaluation time: 0.0007\n",
    "* svr evaluation time: 0.0006\n",
    "* forest evaluation time: 0.002\n",
    "\n",
    "**Discussion**:\n",
    "\n",
    "The final result shows that running one simulation with one sample takes 0.3s or 428x the time of evaluating a model with one sample (0.0007s - ann).\n",
    "\n",
    "The percent error between computed (0.0007) and predicted (0.001) time to evaluate an ANN model was 42%, with the computed value being faster than the predicted value. This is reasonable because computer usage was not controlled for in this experiment, and deviations of 100us can be expected if a computers' processors are in greater use.\n",
    "\n",
    "Using these results an inference will be made to determine the cost of running the simulation 100 times vs running the simulation once, approximating the underlying model and evaluating the model (using the approximation) for the remaining 99 simulations. If the simulation is run 100x, it would take 30s. If the simulation is run once and then the approximation is evaluated the other 99x, it would take 0.3s. **The net result is a speed increase of 100x.**\n",
    "\n",
    "This speed increase is the _highest_ possible outcome for the aforementioned inference. If the time to train the model is included, the speed increase multiplier will decrease because $time_{model} = time_{model-training} + time_{model-evaluation}$. There are two extreme $time_{model}$ scenarios that should be discussed. \n",
    "\n",
    "1. $time_{model-training} >> time_{model-evaluation}$ \n",
    "\n",
    "This scenario likely arises from a user using many data points to train their model, as well a the user sparsely evaluating that model. To compute the speed increase,\n",
    "\n",
    "$speed~increase = \\frac{time_{running-sim} \\cdot n_{simulation}}{time_{model-training}}$\n",
    "\n",
    "2. $time_{model-training} << time_{model-evaluation}$\n",
    "\n",
    "This scenario likely arises from a user heavily leaning on their model to approximate answers. In this case, the speed increase is roughly **100x** as shown in _Discussion - paragraph 3_ .\n",
    "\n",
    "To run a _simple speed test_ :\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "print(\"hi\")\n",
    "end = time.time()\n",
    "_time = end-st_\n",
    "```\n",
    "\n",
    "Table of Contents:\n",
    "* Power flow simulation\n",
    "* Power flow approximation\n",
    "  * Source data\n",
    "  * Analyse data\n",
    "  * Train models\n",
    "  * Model Statistics Including Timing Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single sample power flow simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypsa\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logger = logging.getLogger(\"pypsa\")\n",
    "logger.setLevel(\"WARNING\")\n",
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pypsa\n",
    "from n_dimensional_datasets import *\n",
    "from plotter import *\n",
    "\n",
    "from IPython.display import display # for better Pandas printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder_name = \"/home/ubuntu/Downloads/pypsa/examples/ieee-13/ieee-13-with-load-gen-1-sample/\"\n",
    "\n",
    "network = pypsa.Network(import_name=csv_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample power flow simulation time: 0.3441\n"
     ]
    }
   ],
   "source": [
    "time_hold = []\n",
    "for i in range(10):\n",
    "    st = time.time()\n",
    "    network.pf()\n",
    "    end = time.time()\n",
    "    time_hold.append(end-st)\n",
    "print(\"Single sample power flow simulation time: {:.5}\".format(np.mean(time_hold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>name</th>\n",
       "      <th>Substation</th>\n",
       "      <th>650</th>\n",
       "      <th>646</th>\n",
       "      <th>645</th>\n",
       "      <th>632</th>\n",
       "      <th>633</th>\n",
       "      <th>634</th>\n",
       "      <th>611</th>\n",
       "      <th>684</th>\n",
       "      <th>671</th>\n",
       "      <th>692</th>\n",
       "      <th>675</th>\n",
       "      <th>652</th>\n",
       "      <th>680</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.965503</td>\n",
       "      <td>0.89708</td>\n",
       "      <td>0.900371</td>\n",
       "      <td>0.90997</td>\n",
       "      <td>0.90481</td>\n",
       "      <td>0.829759</td>\n",
       "      <td>0.864038</td>\n",
       "      <td>0.866397</td>\n",
       "      <td>0.870607</td>\n",
       "      <td>0.870589</td>\n",
       "      <td>0.863477</td>\n",
       "      <td>0.861953</td>\n",
       "      <td>0.870607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "name        Substation       650      646       645      632      633  \\\n",
       "name                                                                    \n",
       "2011-01-01         1.0  0.965503  0.89708  0.900371  0.90997  0.90481   \n",
       "\n",
       "name             634       611       684       671       692       675  \\\n",
       "name                                                                     \n",
       "2011-01-01  0.829759  0.864038  0.866397  0.870607  0.870589  0.863477   \n",
       "\n",
       "name             652       680  \n",
       "name                            \n",
       "2011-01-01  0.861953  0.870607  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.buses_t.v_mag_pu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalise_column_names(df, name):\n",
    "        new_columns = []\n",
    "        for column in df.columns:\n",
    "            new_columns.append(name +  \"-\" + str(column))\n",
    "        df.columns = new_columns\n",
    "        return pd.DataFrame(df)\n",
    "\n",
    "def collect_data(path_to_powerflow_data, data):\n",
    "    '''\n",
    "    Assumes folder tree has\n",
    "    path_to_powerflow_data/\n",
    "    -->datafiles\n",
    "    -->results/\n",
    "    '''\n",
    "    data[\"loads\"] = personalise_column_names(pd.read_csv(path_to_powerflow_data + \"loads-p_set.csv\"), \"load\")\n",
    "    data[\"vmags\"] = personalise_column_names(pd.read_csv(path_to_powerflow_data + \"results/\" + \"vmags.csv\"), \"vmag\")\n",
    "    data[\"vangs\"] = personalise_column_names(pd.read_csv(path_to_powerflow_data + \"results/\" + \"vangs.csv\"), \"vang\")\n",
    "    data[\"qmags\"] = personalise_column_names(pd.read_csv(path_to_powerflow_data + \"results/\" + \"qmags.csv\"), \"qmag\")\n",
    "    data[\"linemags\"] = personalise_column_names(pd.read_csv(path_to_powerflow_data + \"results/\" + \"linemags.csv\"), \"linemag\")\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "def set_uniform_sample_size(path_to_powerflow_data, data_to_change, n_samples, seed=None):\n",
    "    '''\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_to_change: list of strings.\n",
    "        ex: [\"loads-p_set\", \"generators-p_max_pu\", \"snapshots\"] \n",
    "    '''\n",
    "\n",
    "    data = {}\n",
    "    for datatype in data_to_change:\n",
    "        data[datatype] = pd.read_csv(path_to_powerflow_data + datatype + \".csv\")\n",
    "\n",
    "    def increase_data(dataframe, n_samples, seed=None):\n",
    "        addon = {}\n",
    "        new_df_list = []\n",
    "        for idx, column in enumerate(dataframe):\n",
    "          \n",
    "            if dataframe[column].dtype == np.float64 or dataframe[column].dtype == np.int64:\n",
    "                ## special cases\n",
    "                if datatype == \"generators-p_max_pu\":\n",
    "                    # add a ridiculous amount of generation so the is always enough power and sim doesn't fail\n",
    "                    addon[column] = np.random.RandomState(seed=seed).uniform(low=100, high=200, size=n_samples)\n",
    "                elif datatype == \"snapshots\":\n",
    "                    addon[column] = np.ones(n_samples)\n",
    "                else:\n",
    "                    addon[column] = np.random.RandomState(seed=seed).uniform(low=0, high=0.5, size=n_samples)\n",
    "            elif dataframe[column].dtype == object:\n",
    "                # assuming object is datetime column\n",
    "                latest_datetime = pd.to_datetime(dataframe[column][0])\n",
    "                addon[column] = []\n",
    "                for sample in range(n_samples):\n",
    "                    addon[column].append(latest_datetime + pd.Timedelta(hours=(1+sample)))\n",
    "            else:\n",
    "                raise TypeError(\"dataframe[column] type: {} should be object or float64/int64\".format(\n",
    "                    type(dataframe[column].dtype)))\n",
    "        addon_dataframe = pd.DataFrame(addon)\n",
    "        return dataframe.head(1).append(addon_dataframe)\n",
    "\n",
    "    for datatype in data:\n",
    "        data[datatype] = increase_data(data[datatype], n_samples-1, seed) # -1 is for original sample, which stays\n",
    "        data[datatype].to_csv(path_to_powerflow_data + datatype + \".csv\", index=False)\n",
    "        print(\"Datatype {} stored\".format(datatype))\n",
    "\n",
    "def create_samples_and_run_network():\n",
    "    '''\n",
    "    assuming all required variables in this function are global and previously defined. Yes lazy I know\n",
    "    '''\n",
    "    set_uniform_sample_size(path_to_powerflow_data, data_to_change, sample_size, seed=None)\n",
    "\n",
    "    network = pypsa.Network(import_name=path_to_powerflow_data)\n",
    "    network.pf()\n",
    "\n",
    "    save_path = path_to_powerflow_data + \"results/\"\n",
    "    network.buses_t.v_mag_pu.to_csv(save_path + \"vmags.csv\")\n",
    "    network.buses_t.v_ang.to_csv(save_path + \"vangs.csv\")\n",
    "    network.buses_t.q.to_csv(save_path + \"qmags.csv\")\n",
    "    network.lines_t.p0.to_csv(save_path + \"linemags.csv\")\n",
    "    \n",
    "def backup_samples(src, dest):\n",
    "    '''\n",
    "    thanks https://www.pythoncentral.io/how-to-recursively-copy-a-directory-folder-in-python/\n",
    "    '''\n",
    "    import os\n",
    "    import errno\n",
    "    import shutil\n",
    "    \n",
    "    try:\n",
    "        if os.path.isdir(dest):\n",
    "            shutil.rmtree(dest)\n",
    "        shutil.copytree(src, dest)\n",
    "        print(\"Backup to {} successful\".format(dest))\n",
    "    except OSError as e:\n",
    "        # If the error was caused because the source wasn't a directory\n",
    "        if e.errno == errno.ENOTDIR:\n",
    "            shutil.copy(src, dest)\n",
    "        else:\n",
    "            print('Directory not copied. Error: %s' % e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USER INPUT\n",
    "sample_size = 1000000\n",
    "\n",
    "data_to_change = [\"loads-p_set\", \"snapshots\", \"loads-q_set\"]\n",
    "\n",
    "path_to_powerflow_example = \"../../pypsa/examples/ieee-13/\"\n",
    "path_to_powerflow_data = (path_to_powerflow_example +\n",
    "                          \"/ieee-13-with-load-gen-uniform-data-\"\n",
    "                          + str(sample_size) +\n",
    "                          \"-samples/\")\n",
    "path_to_powerflow_results = path_to_powerflow_data + \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure [y/n]? This could erase hours worth of datay\n",
      "Backup to ../../pypsa/examples/ieee-13//ieee-13-with-load-gen-uniform-data-1000000-samples/results-backup/ successful\n"
     ]
    }
   ],
   "source": [
    "## Uncomment to generate load samples for modified IEEE-13 network\n",
    "# if sample_size > 10000:\n",
    "#     user = input(\"Are you sure [y/n]? This could erase hours worth of data\")\n",
    "#     if user == \"y\":\n",
    "#         backup_samples(path_to_powerflow_results, path_to_powerflow_data + \"results-backup/\")\n",
    "#         create_samples_and_run_network()\n",
    "# else:\n",
    "#     create_samples_and_run_network()\n",
    "\n",
    "data = {\"loads\": [], \"vmags\": [], \"vangs\": [], \"qmags\": [], \"linemags\": []}\n",
    "collect_data(path_to_powerflow_data, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[\"loads\"].drop(\"load-name\", axis=1)\n",
    "labels = data[\"vmags\"].drop([\"vmag-name\", \"vmag-Substation\"], axis=1) #loc[:,[\"vmag-632\", \"vmag-671\", \"vmag-675\"]]\n",
    "features_and_labels = features.join(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = labels.join(features).corr()\n",
    "\n",
    "# only loads for columns\n",
    "cols = [c for i, c in enumerate(corr_matrix.columns) if corr_matrix.keys().str.contains(\"^load\", regex=True)[i]]\n",
    "reduced_corr_matrix = corr_matrix[cols]\n",
    "reduced_corr_matrix[\"row average\"] = pd.Series(reduced_corr_matrix.mean(axis=1))\n",
    "# only voltages for rows\n",
    "rows = reduced_corr_matrix.index[reduced_corr_matrix.index.str.contains(\"load\")]\n",
    "reduced_corr_matrix.drop(rows, inplace=True)\n",
    "reduced_corr_matrix = reduced_corr_matrix.append(pd.Series(reduced_corr_matrix.mean(), name=\"column average\"))\n",
    "\n",
    "display(reduced_corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.interpolate import LinearNDInterpolator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from custom_transformers import DataFrameSelector, RejectOutliers\n",
    "from sklearn.decomposition import PCA\n",
    "from scoring import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_percentage = 80\n",
    "n_samples = 100000\n",
    "n_training_samples = int(n_samples*(training_percentage/100))\n",
    "\n",
    "# random_seed=None\n",
    "X_train, X_val, y_train, y_val, test_idx, train_idx = train_test_split(features,\n",
    "                                                                       labels,\n",
    "                                                                       range(100000),\n",
    "                                                                       train_size=n_training_samples,\n",
    "                                                                       test_size=n_samples-n_training_samples,\n",
    "                                                                       random_state=None)\n",
    "X_train = X_train.values\n",
    "y_train = y_train.values\n",
    "X_val = X_val.values\n",
    "y_val = y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"y_train: \", y_train.shape)\n",
    "print(\"X_val: \", X_val.shape)\n",
    "print(\"y_val: \", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_stats = {\"n_training_samples\": n_training_samples,\n",
    "                  \"n_validation_samples\": n_samples-n_training_samples,\n",
    "                  \"n_features\": X_train.shape[1],\n",
    "                  \"n_labels\": y_train.shape[1],\n",
    "                  \"interp_run\": False}\n",
    "stats = {\"trainscore\": [], \n",
    "         \"trainscorevar\": [],\n",
    "         \"valscore\": [],\n",
    "         \"valscorevar\": [],\n",
    "         \"rmse\": [],\n",
    "         \"time\": []}\n",
    "approx_type = {\"svr\": copy.deepcopy(stats),\n",
    "               \"rf\": copy.deepcopy(stats),\n",
    "               \"ann\": copy.deepcopy(stats),\n",
    "               \"interp\": copy.deepcopy(stats),\n",
    "               \"linear\": copy.deepcopy(stats)}\n",
    "approx_type[\"interp\"].pop(\"trainscorevar\")\n",
    "approx_type[\"interp\"].pop(\"valscorevar\")\n",
    "approx_type[\"linear\"].pop(\"trainscorevar\")\n",
    "approx_type[\"linear\"].pop(\"valscorevar\")\n",
    "\n",
    "time_start = time.time()\n",
    "### setup approximators\n",
    "\n",
    "\n",
    "## random forest\n",
    "forest = RandomForestRegressor()\n",
    "\n",
    "forest_xval_training_score = cross_val_score(forest, X_train, y_train, cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "forest_xval_val_score = cross_val_score(forest, X_val, y_val, cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "approx_type[\"rf\"][\"trainscore\"].append(forest_xval_training_score.mean())\n",
    "approx_type[\"rf\"][\"trainscorevar\"].append(forest_xval_training_score.std())\n",
    "approx_type[\"rf\"][\"valscore\"].append(forest_xval_val_score.mean())\n",
    "approx_type[\"rf\"][\"valscorevar\"].append(forest_xval_val_score.std())\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "approx_type[\"rf\"][\"rmse\"].append(rmse(forest.predict(X_val), y_val))\n",
    "\n",
    "time_forest = time.time()\n",
    "approx_type[\"rf\"][\"time\"].append(time_forest-time_start)\n",
    "\n",
    "                                \n",
    "## support vector regression\n",
    "if n_samples <= 10000:\n",
    "    n_labels = y_train.shape[1]\n",
    "    svr = copy.deepcopy(stats)\n",
    "    svr_labels = {\"y_train\": None, \"y_val\": None}\n",
    "    for idx in range(n_labels):\n",
    "        svr_labels[\"y_train\"] = y_train.T[idx].T\n",
    "        svr_labels[\"y_val\"] = y_val.T[idx].T\n",
    "        clf = SVR(gamma='scale', C=1.0, epsilon=0.0002, kernel='rbf')\n",
    "        '''\n",
    "        Scikit-Learn cross-validation features expect a utility function (greater is better) rather than a cost function\n",
    "        (lower is better), so the scoring function is actually the opposite of the MSE (i.e., a negative value), \n",
    "        which is why the preceding code computes -scores before calculating the square root.\n",
    "        - A. Geron, Hands on Machine Learning pg 101 \n",
    "        '''\n",
    "        svr_xval_training_score = cross_val_score(clf, X_train, svr_labels[\"y_train\"], cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "        svr_xval_val_score = cross_val_score(clf, X_val, svr_labels[\"y_val\"], cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "        svr[\"trainscore\"].append(svr_xval_training_score.mean())\n",
    "        svr[\"trainscorevar\"].append(svr_xval_training_score.std())\n",
    "        svr[\"valscore\"].append(svr_xval_val_score.mean())\n",
    "        svr[\"valscorevar\"].append(svr_xval_val_score.std())\n",
    "\n",
    "        clf.fit(X_train, svr_labels[\"y_train\"])\n",
    "        svr[\"rmse\"].append(rmse(clf.predict(X_val), svr_labels[\"y_val\"]))\n",
    "\n",
    "        time_svr = time.time()\n",
    "        svr[\"time\"].append(time_svr - time_forest)\n",
    "\n",
    "        if idx == (n_labels-1):\n",
    "            display(\"rmse: \", svr[\"rmse\"])\n",
    "            label_results = pd.DataFrame(columns=labels.columns)\n",
    "            label_results.loc[0] = svr[\"trainscore\"]\n",
    "            display(label_results)\n",
    "\n",
    "    approx_type[\"svr\"][\"trainscore\"].append(np.mean(svr[\"trainscore\"]))\n",
    "    approx_type[\"svr\"][\"trainscorevar\"].append(np.mean(svr[\"trainscorevar\"]))\n",
    "    approx_type[\"svr\"][\"valscore\"].append(np.mean(svr[\"valscore\"]))\n",
    "    approx_type[\"svr\"][\"valscorevar\"].append(np.mean(svr[\"valscorevar\"]))\n",
    "    approx_type[\"svr\"][\"rmse\"].append(np.mean(svr[\"rmse\"]))\n",
    "    approx_type[\"svr\"][\"time\"].append(np.mean(svr[\"time\"]))\n",
    "\n",
    "\n",
    "## ann\n",
    "hidden_layer_size = 100\n",
    "ann = MLPRegressor(hidden_layer_sizes=(hidden_layer_size, hidden_layer_size),\n",
    "                   activation=\"logistic\",\n",
    "                   solver=\"adam\",\n",
    "                   alpha=0.000001,\n",
    "                   learning_rate_init=1e-5,\n",
    "                   max_iter=100000,\n",
    "                   n_iter_no_change=100,\n",
    "                   tol=1e-9,\n",
    "                  )\n",
    "\n",
    "ann_xval_training_score = cross_val_score(ann, X_train, y_train, cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "ann_xval_val_score = cross_val_score(ann, X_val, y_val, cv=5, n_jobs=-1, scoring=make_scorer(r2_score))\n",
    "approx_type[\"ann\"][\"trainscore\"].append(ann_xval_training_score.mean())\n",
    "approx_type[\"ann\"][\"trainscorevar\"].append(ann_xval_training_score.std())\n",
    "approx_type[\"ann\"][\"valscore\"].append(ann_xval_val_score.mean())\n",
    "approx_type[\"ann\"][\"valscorevar\"].append(ann_xval_val_score.std())\n",
    "print(\"\\n\\nANN training scores: {}\\n\\n val scores: {}\".format(ann_xval_training_score, ann_xval_val_score))\n",
    "\n",
    "ann.fit(X_train, y_train)\n",
    "approx_type[\"ann\"][\"rmse\"].append(rmse(ann.predict(X_val), y_val))\n",
    "\n",
    "time_ann = time.time()\n",
    "approx_type[\"ann\"][\"time\"].append(time_ann-time_svr)\n",
    "\n",
    "\n",
    "## interpolation\n",
    "# interp training gets very slow as the number of features grows\n",
    "if X_train.shape[1] < 4:\n",
    "    training_stats[\"interp_run\"] = True\n",
    "    interp = LinearNDInterpolator(X_train, y_train, fill_value=0)\n",
    "\n",
    "    time_interp = time.time()\n",
    "    approx_type[\"interp\"][\"time\"].append(time_interp-time_ann)\n",
    "\n",
    "    approx_type[\"interp\"][\"trainscore\"].append(r2_score(y_train, interp(X_train)))\n",
    "    approx_type[\"interp\"][\"valscore\"].append(r2_score(y_val, interp(X_val)))\n",
    "    approx_type[\"interp\"][\"rmse\"].append(rmse(interp(X_val), y_val))\n",
    "    \n",
    "## linear regression\n",
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train)\n",
    "\n",
    "time_linear = time.time()\n",
    "try:\n",
    "    approx_type[\"linear\"][\"time\"].append(time_linear-time_interp)\n",
    "except:\n",
    "    approx_type[\"linear\"][\"time\"].append(time_linear-time_ann)\n",
    "\n",
    "approx_type[\"linear\"][\"trainscore\"].append(r2_score(y_train, linear.predict(X_train)))\n",
    "approx_type[\"linear\"][\"valscore\"].append(r2_score(y_val, linear.predict(X_val)))\n",
    "approx_type[\"linear\"][\"rmse\"].append(rmse(linear.predict(X_val), y_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Statistics Including Timing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print and save stats\n",
    "import csv\n",
    "import datetime\n",
    "datetimestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "print(\"Training Stats\\n\\n{}\".format(pd.DataFrame(training_stats, index=[0])))\n",
    "for t in approx_type:        \n",
    "    print(\"\\n\", t + \" Stats: \\n\")\n",
    "    results_to_save = pd.DataFrame(approx_type[t]).round(3)\n",
    "    display(results_to_save)\n",
    "    results_to_save.to_csv(\"model_results/{}-{}_samples-resuls-{}.csv\".format(t, n_samples, datetimestamp))\n",
    "    \n",
    "# with open(\"results-{}_samples-{}.csv\".format(datetimestamp), 'w') as f:\n",
    "#     for key in approx_type:\n",
    "#         f.write(\"%s,%s\\n\"%(key, value))\n",
    "\n",
    "if n_samples <= 10000:\n",
    "    print(\"svr training score - non crossvalidation: \", r2_score(svr_labels[\"y_train\"], clf.predict(X_train)))\n",
    "    print(\"svr validation score - non crossvalidation: \", r2_score(svr_labels[\"y_val\"], clf.predict(X_val)))\n",
    "print(\"rf training score - non crossvalidation: \", r2_score(y_train, forest.predict(X_train)))\n",
    "print(\"rf validation score - non crossvalidation: \", r2_score(y_val, forest.predict(X_val)))\n",
    "print(\"ann training score - non crossvalidation: \", r2_score(y_train, ann.predict(X_train)))\n",
    "print(\"ann validation score - non crossvalidation: \", r2_score(y_val, ann.predict(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_evaluation_time_start = time.time()\n",
    "ann.predict(X_val[0].reshape(1,9))\n",
    "ann_evaluation_time_stop = time.time()\n",
    "print(\"ANN evaluation time: {:.5}\".format(ann_evaluation_time_stop-ann_evaluation_time_start))\n",
    "\n",
    "svr_evaluation_time_start = time.time()\n",
    "clf.predict(X_val[0].reshape(1,9))\n",
    "svr_evaluation_time_stop = time.time()\n",
    "print(\"svr evaluation time: {:.5}\".format(svr_evaluation_time_stop-svr_evaluation_time_start))\n",
    "\n",
    "forest_evaluation_time_start = time.time()\n",
    "forest.predict(X_val[0].reshape(1,9))\n",
    "forest_evaluation_time_stop = time.time()\n",
    "print(\"forest evaluation time: {:.5}\".format(forest_evaluation_time_stop-forest_evaluation_time_start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
